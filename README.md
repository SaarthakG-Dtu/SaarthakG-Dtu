<!-- ================= HEADER ================= -->

<h1 align="center">Saarthak Gupta</h1>
<h3 align="center">Aspiring AI Researcher | Computer Vision | Multimodal Perception | World Modelling</h3>

<p align="center">
  <img src="https://readme-typing-svg.demolab.com/?lines=AI+Research+%7C+Computer+Vision;Multimodal+Perception+Systems;Real-World+Intelligent+AI;Exploring+Reasoning+and+World+Models&center=true&width=900&height=45">
</p>

<p align="center">
  <img src="https://komarev.com/ghpvc/?username=SaarthakG-Dtu&label=Profile+Views&style=for-the-badge"/>
</p>

<p align="center">
  <img src="https://media.giphy.com/media/qgQUggAC3Pfv687qPC/giphy.gif" width="420"/>
</p>

---

## üå± About Me

I am a B.Tech student in Mathematics & Computing at Delhi Technological University (DTU) with a strong interest in **Computer Vision, Multimodal AI, and real-world perception systems**.

My primary focus lies in building intelligent systems that can perceive complex environments, fuse multimodal signals, and move toward more robust world modelling and reasoning capabilities. I am particularly drawn to research at the intersection of deep learning, perception, and real-world deployment.

---

## üî¨ Research Interests

* Computer Vision & Scene Understanding
* Multimodal Models (Vision + Sensor Fusion)
* AI for Real-World Perception Systems
* World Modelling and Reasoning in AI
* Autonomous and Aerial Intelligence

---

## üöÅ Research Exposure

### UAS-DTU | Software Team ‚Äî DARPA TRIAGE Challenge (2025)

Contributed to the development of a real-time multimodal perception system for medical triaging in a high-stakes, real-world setting.

My work involved exploring perception pipelines that integrate vision and motion signals, with emphasis on:

* LiDAR and depth-based signal fusion
* YOLO-based segmentation for region-level perception
* Dense pixel tracking for motion understanding
* Temporal modelling using LSTM for fine-grained motor activity analysis

This experience significantly shaped my interest in multimodal perception and research-driven AI systems deployed beyond controlled lab environments.

---

## üõ†Ô∏è Technical Stack

### Programming

* Python (Primary)
* C++ (Foundational)

### Machine Learning & Deep Learning

* PyTorch
* Scikit-Learn
* Hugging Face Transformers
* CNNs, RNNs, UNet Architectures

### Computer Vision & Perception

* OpenCV
* YOLO (Detection & Segmentation)
* Semantic Segmentation Pipelines
* Image Processing & Feature Extraction

### Data & Scientific Computing

* NumPy
* Pandas

### Academic Foundations

* Data Structures
* Discrete Mathematics
* Probability & Statistics
* Calculus and Real Analysis

---

## üöÄ Selected Projects

### üõ∞Ô∏è Semantic Segmentation on Aerial Drone Imagery

Developed a high-resolution semantic segmentation pipeline tailored for aerial datasets with large image sizes and fine-grained object distribution.

* Implemented a UNet-based architecture with a transfer learning backbone in PyTorch
* Designed a custom patching and reconstruction workflow to preserve spatial resolution in gigapixel-scale imagery
* Addressed class imbalance using a hybrid Dice + Cross-Entropy loss
* Optimized convergence using AdamW and OneCycleLR scheduling
  The project focused on robust detection of small objects such as vehicles and pedestrians in complex aerial scenes.

---

### üçé Vision-Based Fruit Counting using YOLOv8 and OpenCV

Built a dual-view fruit detection and counting system leveraging geometric alignment and color-aware detection.

* Used ORB feature matching and RANSAC for homography-based image alignment
* Applied HSV color masking for improved multi-color fruit detection
* Designed a custom matching logic using spatial proximity and Euclidean distance thresholds
* Integrated YOLOv8 for reliable object detection across viewpoints
  This project emphasized practical computer vision system design beyond standard single-image inference.

---

### üß© Multimodal Perception Experiments (DARPA-Oriented Work)

Explored perception architectures that combine motion, depth, and visual cues for real-time understanding.

* Implemented dense pixel tracking for motion feature extraction
* Engineered displacement and angular flow-based features for temporal modelling
* Developed LSTM-based classifiers for fine-grained motor signal interpretation
* Investigated sensor fusion strategies for real-world perception pipelines
  These explorations strengthened my inclination toward research in multimodal and real-time AI systems.

---

## üìà GitHub Activity & Analytics

<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=SaarthakG-Dtu&show_icons=true&theme=tokyonight&hide_border=true" height="165"/>
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=SaarthakG-Dtu&theme=tokyonight&hide_border=true" height="165"/>
</p>

<p align="center">
  <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=SaarthakG-Dtu&layout=compact&theme=tokyonight&hide_border=true"/>
</p>

---

## üéì Education

**B.Tech in Mathematics & Computing**
Delhi Technological University (DTU)

---

## üéØ Current Focus

* Strengthening theoretical foundations in Machine Learning and Mathematics
* Advancing in Computer Vision and Multimodal Perception
* Building research-oriented AI projects with real-world relevance
* Preparing for AI Research Internships and MS in AI / Robotics

---

## üåç Connect

* Email: [saarthakgupta2006@gmail.com](mailto:saarthakgupta2006@gmail.com)
* GitHub: https://github.com/SaarthakG-Dtu
* LinkedIn: (Add your LinkedIn profile)

---

<p align="center">
Focused on perception-driven AI, with the long-term goal of contributing to research in intelligent real-world systems.
</p>
